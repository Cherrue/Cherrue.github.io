# 7장. 분산 시스템을 위한 유일 ID 생성기 설계

## 1. 유일 ID 생성 방법

### 1-1. 다중 마스터 복제

id를 auto increase 하되, 1이 아닌 서버의 대수 만큼씩 증가

장점

- ID 유일성 보장
- 서버의 대수를 늘리면 초당 생산 가능 ID 수를 늘릴 수 있음

단점

- 여러 데이터 센터에 걸쳐 규모를 늘리기 어렵다
- ID가 시간과 비례함을 보장할 수 없다
- 서버를 추가하거나 삭제할 때 잘 동작하기 어렵다

### 1-2. UUID

각 서버의 ID 생성기가 128비트 랜덤 문자열 부여

장점

- 서버 사이의 조율이 필요 없어 동기화 이슈가 없음
- 각 서버가 각자 ID 생성기를 갖기 때문에 규모 확장이 쉽다

단점

- 길이가 길다.
- ID가 시간과 비례하지 않는다
- 숫자가 아닌 문자열이다

### 1-3. 티켓 서버

auto increase로 ID를 발행하는 티켓 서버를 구축하고 DB는 이 티켓서버를 통해 ID를 받는다

장점

- 요구사항을 만족시키는 ID를 만들기 쉽다
- 구현이 쉽다

단점

- 티켓서버가 SPOF 가 된다.
- 티켓 서버 이중화 시 동기화 이슈가 있다.

### 1-4. 트위터 스노플레이크

ID를 비트 단위로 의미를 부여한다.

buffer + timestamp + data center ID + server ID + 일련번호

장점

- 요구사항을 만족시키는 ID를 만들기 쉽다

단점

- 요구사항을 제대로 식별해야 한다
- ID의 형태를 변경하기 어렵다.

## 2. 단계별 설계

### 1단계 문제 이해 및 설계 범위 확정

- 유일성
- 자료형
- 크기
- 정렬
- 단위 시간 당 생산 가능 개수

### 2단계 개략적인 설계안 제시 및 동의 구하기

- ID 생성 기법 선택

### 3단계 상세 설계

- 선택한 기법의 단위 시간당 ID 생산량 계산
- 해당 ID로 저장 가능한 최대 데이터 수
- 시간 값이 들어간다면, 저장 가능한 기간

### 4단계 마무리

추가 논의 사항

- 시계 동기화 : NTP 서버 구축
- 스노플레이크라면) 각 section 의 크기 최적화
- 고가용성 : 장애 대응 방법

# 8장. URL 단축기 설계

## 1. URL 단축기

### 1-1. 개념

긴 URL을 domain.com/{특정 길이의 문자열} 로 매핑하여 리다이렉트 해주는 서비스

### 1-2. 전제

- 입력으로 주어지는 URL이 다르면 결과 값도 달라야 한다
- 계산된 단축 값은 원래 URL로 복원되어야 한다.

## 2. URL 단축 방법

### 2-1. 해시 후 충돌 해소

해시 함수를 통해 url을 해시 값으로 바꾸고, 충돌 발생 시 충돌을 해결하는 방법

**필요한 설정**

- 해시의 길이 : 요구사항에 맞도록 선정
- 해시 함수 : CRC32, MD5, SHA-1 등 잘 알려진 함수를 쓰면 된다.
- 충돌 해소 : 해시 충돌이 발생하면 원래 URL에 사전에 정의한 문자열을 뒤에 붙여 다시 해시

**단점**

- 충돌의 발생을 DB 조회를 해야 알기 때문에 매 생성마다 DB 조회가 필수
    
    -> 데이터베이스 대신 블룸 필터를 사용하면 성능을 높일 수 있다
    

### 2-2. base-N 변환

URL을 숫자에 매칭하여 그걸 base-N = N진수의 수로 변환

**단점**

- URL의 길이에 따라 결과 길이가 가변적
- 유일성 보장 ID 생성기가 필요
- 생성기의 규칙이 뻔히 보이면 보안 문제 발생

## 3. 리디렉션 방법

### 3-1. 301 Permanently moved

해당 응답으로 리디렉션하면 브라우저가 이 응답을 캐시하여 다음 번 요청부터는 URL 단축기 서버에 가지 않음

### 3-2. 302 Found

이번 요청의 리디렉션만 유효. 같은 요청을 재용청하더라도 URL 단축기 서버에 가야 함

## 4. 단계별 설계

### 1단계 문제 이해 및 설계 범위 확정

**요구사항 식별**

- URL 생성 트래픽 규모
- 줄인 URL의 길이
- URL에 포함 가능한 문자 종류
- 생성한 URL의 갱신, 삭제 가능성
- 생성한 URL의 읽기 트래픽 규모
- 서비스 (예정) 기간
- 용량 추정 : url의 길이 * 생성 트래픽 규모 * 서비스 예정 기간

### 2단계 개략적 설계안 제시 및 동의 구하기

**URL 단축 서버의 주요 기능**

- URL 단축
- API 엔드포인트 : 생성 API, 읽기 API
- 리디렉션 : 301과 302 중 선택.
    
    → 301의 사용 : 서버 부하가 줄어들지만, 사용량 분석, 발생 위치 등의 트래픽 분석 불가
    
    → 302의 사용 : 서버 부하가 늘지만, 트래픽 분석에 강함
    

### 3단계 상세 설계

**데이터의 저장**

- 해시 테이블을 메모리에 저장 : 큰 서비스에 부적합
- 데이터베이스에 저장

**해시 함수 설계**

- URL에 사용 가능한 문자 개수 ^ 줄인 URL의 길이 = 매핑 가능한 URL의 개수
- URL 단축 방법의 선택 필요
- base-N 변환을 선택하면, ID 생성기를 개발해야 한다. 7장의 스노우플레이크가 좋겠지

**리디렉션 설계**

- 자주 호출되는 URL을 캐싱

### 4단계 마무리

**추가 고민**

- 처리율 제한 장치 : 단축 요청을 악의적으로 많이 보내올 수 있으니 방어
- 웹 서버의 규모 확장 : 이 서버는 stateless 한 웹서버로 개발이 가능해 규모 확장이 자유로움
- 데이터 분석 솔루션 : 어떤 링크를 얼마나 많은 사용자가 사용하는가 등
- 가용성, 데이터 일관성, 안정성

# 9장. 웹 크롤러 설계

## 웹 크롤러 활용

- 검색 엔진 인덱싱 : 검색 엔진을 위한 로컬 인덱스 생성 (구글봇)
- 웹 아카이빙 : 웹 정보를 장기 보관 (미국 국회 도서관)
- 웹 마이닝 : 인터넷에서 유용한 자료를 도출 (금융기업의 주주 총회 자료 크롤링 등)
- 웹 모니터링 : 저작권이나 상표권이 침해되는 사례 모니터링 (디지마크)

## 단계별 설계

### 1단계 문제 이해 및 설계 범위 확정

**문제 이해**

1. URL 집합이 입력되면 해당 모든 웹페이지를 다운로드
2. 다운받은 웹페이지에서 URL을 추출
3. 추출된 URL을 다시 URL 집합으로 추가하고 다시 1로

**설계 범위 확정**

- 용도
- 단위 기간 당 수집량
- 웹페이지의 갱신 또는 신규 웹페이지 수집 여부
- 수집 웹페이지 저장 여부
- 저장 기간
- 중복된 컨텐츠의 처리

**고려사항**

- 규모확장성 : 양이 너무 많으니 병행성을 활용해야 함
- 안정성 : 웹에는 크롤러 함정, 잘못된 HTML, 응답하지 않는 서버, 악성코드 등이 많아 대응이 필요
- 예절 : 한 사이트에 짧은 시간동안 너무 많은 요청을 보내면 안 된다
- 확장성 : HTML이 아닌 새로운 형태의 콘텐츠(PNG 등)를 지원하기 쉬워야 한다

**개략적 규모추정**

- QPS = 단위 기간 당 수집량을 초로 환산
- 필요 저장용량 = 단위 기간 당 수집량을 1년으로 환산 * 평균 웹페이지 크기 * 저장 기간

### 2단계 개략적 설계안 제시 및 동의 구하기

**시작 URL 집합**

- 지역적인 특색
- 주제별로 시작 URL을 추가
- 특정하기는 어렵다. 의도가 무엇인지를 전달하는 것이 중요

**미수집 URL 저장소**

FIFO 형태의 큐에 수집할 URL 저장

**HTML 다운로더**

원하는 웹 페이지를 다운로드

**도메인 이름 변환기**

도메인을 IP로 변환하는 장치. 미리 구축해둔다

**콘텐츠 파서**

- 파싱 : 웹 페이지 내용을 읽어옴
- 검증 : 문제가 있는 웹 페이지인지 검증

동작이 느린 컴포넌트이므로 독립적으로 실행

**중복 컨텐츠 ?**

공개된 자료에 의하면 29%의 웹 콘텐츠는 중복된다.

저장 공간을 최소화하기 위해 해시를 이용한 중복 검사 실행

**콘텐츠 저장소**

- 대부분 디스크에 저장
- 인기 있는 콘텐츠는 메모리에 올려둠

**URL 추출기**

웹 페이지 내 URL을 추출해 절대경로로 변환

**URL 필터**

등록된 URL blacklist 해당하는지 검사

**이미 방문한 URL ?**

무한 루프와 서버 부하를 줄이기 위해 방문한 URL인지 검사

보통 블룸필터나 해시 테이블을 쓴다

**URL 저장소**

이미 방문한 URL을 저장

### 3단계 상세 설계

#### **순회 방식**

BFS : 웹의 깊이는 굉장히 깊기 때문에 DFS가 아닌 BFS 사용

**문제점**

- 예의 없는 크롤러 : 한 페이지에서 추출한 URL은 대부분 같은 URL이다. 이를 병렬로 처리하면 예의가 없다
- 페이지는 공평하지 않음 : 페이지 간에도 품질 차이가 있다. BFS는 우선순위가 없어 우선순위 적용이 필요

#### 미수집 URL 저장소

**예의**

- 큐 라우터 : 동일 웹 페이지에 병렬로 보내지 않기 위해 같은 웹 페이지는 하나의 큐에 저장하도록 라우팅
- 매핑 테이블 : 호스트 이름과 큐 이름을 매핑
- 큐 선택기 : 큐에서 나온 URL을 지정한 스레드로 보내줌
- 작업 스레드 : 순차 처리 + 작업 중간 delay로 예의 범절 찾아버리기

**우선순위**

- 순위결정장치 : Page rank, 트래픽 양, 갱신 빈도 등으로 우선순위를 결정해 큐 라우팅
- 큐 선택기 : 우선 순위가 높은 페이지가 담기는 큐를 더 자주 뽑아서 작업을 수행

**신선도**

- 웹 페이지의 이전 갱신 빈도를 보고, 자주 갱신되는 페이지는 자주 재수집
- 변경 이력 저장 필요
- 중요한 페이지의 재수집 우선순위 증가

**미수집 URL 저장소의 저장장치**

- 미수집 URL의 데이터는 큰데, 접근은 잦다
- 그래서 하드에다가 저장한 후 일부를 떼서 메모리에 한 번에 적재해서 소비

#### HTML 다운로더

**Robots.txt**

웹사이트와 크롤러가 소통하는 표준. 수집해도 되는 페이지 목록이 작성되어 있음

계속해서 다시 저장하는 것을 방지하기 위해 보통 캐시에 저장

**성능 최적화**

- 분산 크롤링
- 도멩 이름 변환 결과 캐시 : DNS 결과를 받는 시간이 병목이 됨. DNS 요청은 한 번에 하나만 되기 때문
- 지역성 : 크롤링 서버를 해당 지역 근처에 설치
- 짧은 타임 아웃 : 이상한 웹사이트가 많아서 조금만 응답이 늦어도 바로 버림

**안정성**

- 안정해시 : 다운로더 서버의 추가 삭제가 용이함
- 크롤링 상태 및 수집 데이터 저장 : 서버가 자주 죽기 때문에 상태를 자주자주 디스크에 써주어야 한다
- 예외 처리 : 예외가 발생해도 전체 서버에 장애가 나지 않도록 주의
- 데이터 검증 : 오류 발생을 줄이기 위한 수단

**확장성**

- 웹페이지만 다운로드 하다가 이미지도 다운받고 싶을 수 있으니까 인터페이스처럼 만들어야겠지

**문제 있는 콘텐츠 감지 및 회피**

- 중복 콘텐츠 : 해시나 체크섬 사용
- 거미 덫 : 크롤러를 무한 루프에 빠지게끔 설계된 웹페이지. URL의 길이 제한, 너무 많은 페이지를 가진 호스트 제한, blacklist 작성

**데이터 노이즈**

- 광고, 스팸, 그냥 스크립트 코드 등은 가치가 없어서 그냥 버려도 된다

### 4단계 마무리

추가 논의사항

- 서버 측 렌더링 : 위의 방법들은 정적 페이지를 수집하는 것. 동적 페이지 수집을 위해선 서버측 렌더링으로 그려주어야 수집이 가능
- 원치 않은 페이지 필터링 : 스팸 방지 필터링, 스팸성 페이지 필터링
- 데이터베이스 다중화 및 샤딩 : 데이터 계층의 안정성, 확장성, 가용성 강화
- 수평적 규모 확장성 : stateless 서버로 구성
- 가용성, 일관성, 안정성 : 대규모 시스템의 기본! 어떻게 해야하는지 잊었다면 1장을 다시 볼 것
- 데이터 분석 솔루션 : 모니터링을 통한 성능 강화를 꾀함
